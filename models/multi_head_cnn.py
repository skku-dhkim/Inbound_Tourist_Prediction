# -*- coding: utf-8 -*-
"""Multi_Head_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YHFjZA1rDY_ZX8n1OYXi2l1Jw15Xfbf6
"""

import tensorflow as tf
import numpy as np
from keras import activations
from keras.models import Model

from keras import layers
from keras import Sequential 
from keras.layers import Conv1D, Dense, Flatten, MaxPool1D, Input, concatenate, Dropout
from keras import Model

class FiveMultiHead(Model):
    def __init__(self, units_1, units_2, vector_size, features, outputs_sequence, dropout=0.25):
        super(FiveMultiHead, self).__init__()
        self.conv1d_1 = Conv1D(units_2, 2, activation = activations.relu)
        self.maxpool_1 = MaxPool1D(pool_size=2)
        self.flatten_1 = Flatten()

        self.conv1d_2 = Conv1D(units_2, 2, activation = activations.relu)
        self.maxpool_2 = MaxPool1D(pool_size=2)
        self.flatten_2 = Flatten()

        self.conv1d_3 = Conv1D(units_1, 2, activation = activations.relu)
        self.maxpool_3 = MaxPool1D(pool_size=2)
        self.flatten_3 = Flatten()

        self.conv1d_4 = Conv1D(units_1, 2,activation = activations.relu)
        self.maxpool_4 = MaxPool1D(pool_size=2)
        self.flatten_4 = Flatten()

        self.conv1d_5 = Conv1D(units_1, 2,activation = activations.relu)
        self.maxpool_5 = MaxPool1D(pool_size=2)
        self.flatten_5 = Flatten()

        self.dense_1 = Dense(100, activation = activations.relu)
        self.dense_2 = Dense(outputs_sequence, activation = activations.relu)

        self.vector_size = vector_size
        self.features = features

    def call(self, input_X, **kwargs):

        x1 = self.conv1d_1(input_X[0])
        x1 = self.maxpool_1(x1)
        x1 = self.flatten_1(x1)

        x2 = self.conv1d_2(input_X[1])
        x2 = self.maxpool_2(x2)
        x2 = self.flatten_2(x2)

        x3 = self.conv1d_3(input_X[2])
        x3 = self.maxpool_3(x3)
        x3 = self.flatten_3(x3)

        x4 = self.conv1d_4(input_X[3])
        x4 = self.maxpool_4(x4)
        x4 = self.flatten_4(x4)

        x5 = self.conv1d_5(input_X[4])
        x5 = self.maxpool_5(x5)
        x5 = self.flatten_5(x5)

        concat = concatenate([x1, x2, x3, x4, x5])
        result = self.dense_1(concat)
        final_result = self.dense_2(result)

        return final_result


class Attention(tf.keras.layers.Layer):
    def __init__(self, units):
        super(Attention, self).__init__()
        self.W1 = tf.keras.layers.Dense(units)
        self.W2 = tf.keras.layers.Dense(units)

        self.V = tf.keras.layers.Dense(1)
        self.flatten = Flatten()

    def call(self, Q, K, values):
        score = self.V(tf.nn.tanh(
            self.W1(Q) + self.W2(K)))

        attention_weights = tf.nn.softmax(score, axis=1)

        context_vector = attention_weights * values
        context_vector = self.flatten(context_vector)

        return context_vector

class FiveMultiHead_Attention(Model):
    def __init__(self, size_1,size_2, vector_size, features, outputs_sequence, dropout=0.25):
      super(FiveMultiHead_Attention, self).__init__()
      self.vector_size = vector_size
      self.features = features
      self.conv1d_1 = Conv1D(size_1,2, activation = activations.relu)
      self.maxpool_1 = MaxPool1D(pool_size=2)
      self.conv1d_2 = Conv1D(size_1,2, activation = activations.relu)
      self.maxpool_2 = MaxPool1D(pool_size=2)
      self.conv1d_3 = Conv1D(size_1,2, activation = activations.relu)
      self.maxpool_3 = MaxPool1D(pool_size=2)
      self.conv1d_4 = Conv1D(size_2,2,activation = activations.relu)
      self.maxpool_4 = MaxPool1D(pool_size=2)
      self.conv1d_5 = Conv1D(size_2,2,activation = activations.relu)
      self.maxpool_5 = MaxPool1D(pool_size=2)
      self.flatten = Flatten()
      self.dense_1 = Dense(100, activation= activations.relu)
      self.dense_2 = Dense(outputs_sequence, activation = activations.relu)
      self.attention_1 = Attention(16)
      self.attention_2 = Attention(16)
      self.attention_3 = Attention(16)
      self.attention_4 = Attention(16)
      self.attention_5 = Attention(16)
      self.dropout = Dropout(dropout)

    def call(self, input_X, **kwargs):

      x0 = self.conv1d_1(input_X[0])
      x0 = self.maxpool_1(x0)

      x1 = self.conv1d_2(input_X[1])
      x1 = self.maxpool_2(x1)

      x2 = self.conv1d_3(input_X[2])
      x2 = self.maxpool_3(x2)

      x3 = self.conv1d_4(input_X[3])
      x3 = self.maxpool_4(x3)

      x4 = self.conv1d_5(input_X[4])
      x4 = self.maxpool_5(x4)

      concat_1 = concatenate([x0,x1,x2,x3,x4])

      attention_0 = self.attention_1(x0,x0, x0)
      attention_1 = self.attention_2(x1,x1, x1)
      attention_2 = self.attention_3(x2,x2, x2)
      attention_3 = self.attention_4(x3,x3, x3)
      attention_4 = self.attention_5(x4,x4, x4)

      concat_2 = concatenate([self.flatten(concat_1), attention_0, attention_1, attention_2,
                              attention_3, attention_4])

      x = self.dense_1(concat_2)
      _x = self.dense_2(x)

      return _x